% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MRF.R
\name{M_RF}
\alias{M_RF}
\title{M-Learner with RF}
\usage{
M_RF(feat, tr, yobs, nthread = 0, verbose = FALSE,
  mu.forestry = list(relevant.Variable = 1:ncol(feat), ntree = 1000,
  replace = TRUE, sample.fraction = 0.8, mtry = round(ncol(feat) * 13/20),
  nodesizeSpl = 2, nodesizeAvg = 1, splitratio = 1, middleSplit = TRUE),
  e.forestry = list(relevant.Variable = 1:ncol(feat), ntree = 500,
  replace = TRUE, sample.fraction = 0.5, mtry = ncol(feat), nodesizeSpl =
  11, nodesizeAvg = 33, splitratio = 0.5, middleSplit = FALSE),
  tau.forestry = list(relevant.Variable = 1:ncol(feat), ntree = 1000,
  replace = TRUE, sample.fraction = 0.7, mtry = round(ncol(feat) * 17/20),
  nodesizeSpl = 5, nodesizeAvg = 6, splitratio = 0.8, middleSplit = TRUE))
}
\arguments{
\item{feat}{A data frame containing the features.}

\item{tr}{A numeric vector with 0 for control and 1 for treated variables.}

\item{yobs}{A numeric vector containing the observed outcomes.}

\item{nthread}{Number of threads which should be used to work in parallel.}

\item{verbose}{TRUE for detailed output, FALSE for no output.}

\item{mu.forestry, tau.forestry, e.forestry}{A list containing the
hyperparameters for the \code{forestry} package that are used for
estimating the response functions, the CATE, and the propensity score.
These hyperparameters are passed to the \code{forestry} package. Please
refer to the \href{https://github.com/soerenkuenzel/forestry}{forestry}
package for a more detailed documentation of the hyperparamters.
\itemize{
   \item \code{relevant.Variable} Variables that are only used at the first 
         stage.
   \item \code{ntree} Numbers of trees used at the first stage.
   \item \code{replace} Sample with or without replacement at the first 
         stage.
   \item \code{sample.fraction} The size of total samples to draw for the 
         training data in the first stage.
   \item \code{mtry} The number of variables randomly selected at each 
         splitting point.
   \item \code{nodesizeSpl} minimum nodesize at the first stage for 
         the observations in the splitting set. (see the details of the 
         \code{forestry} package)
   \item \code{nodesizeAvg} minimum nodesize at the first stage for 
         the observations in the averaging set.
   \item \code{splitratio} Proportion of the training data used as the 
         splitting dataset at the first stage.
   \item \code{middleSplit} If true, the split value will be exactly in the 
         middle between two observations. Otherwise, it will take a point 
         based on a uniform distribution between the two observations. 
}}
}
\value{
An object from a class that is derived from the \code{CATE-estimator}
  class. It should be used with one of the following functions;
  \code{EstimateCATE}, \code{CateCI}, \code{CateBIAS},
  and \code{EstimateAllSampleStatistics}. The object has at least the
  following slots:
  \item{\code{feature_train}}{A copy of feat.}
  \item{\code{tr_train}}{A copy of tr.}
  \item{\code{yobs_train}}{A copy of yobs.}
  \item{\code{creator}}{Function call that creates the CATE estimator. This
  is used for different bootstrap procedures.}
}
\description{
This is an implementation of the X-learner with Random
Forests (Breiman 2001) at the first and second stage. The function returns an X-RF object.
}
\details{
The M-Learner estimates the CATE in two steps:
\enumerate{
 \item
    Estimate the response functions and the propensity score:
    \deqn{\mu_0(x) = E[Y(0) | X = x]}
    \deqn{\mu_1(x) = E[Y(1) | X = x]} 
    \deqn{e(x) = E[W | X = x]} 
    using the base learner and denote the estimates as \eqn{\hat \mu_0},
    \eqn{\hat \mu_1}, and \eqn{\hat e}.
 \item
    Define the adjusted modified outcomes as 
    \deqn{ R _i = (Z_i - \hat e(x_i)) / (\hat e(x_i)[1 - \hat e(x_i)]) 
    (Y_i - \hat \mu_1(x_i) [1 - \hat e(x_i)] - \hat \mu_0(x_i)\hat e(x_i)).}
    Now employ the base learner to estimate
    \deqn{\tau(x) = E[R | X = x].} 
    The result is the CATE estimator.
    }
}
\examples{
require(causalToolbox)

# create example data set
simulated_experiment <- simulate_causal_experiment(
  ntrain = 1000,
  ntest = 1000,
  dim = 10
)
feat <- simulated_experiment$feat_tr
tr <- simulated_experiment$W_tr
yobs <- simulated_experiment$Yobs_tr
feature_test <- simulated_experiment$feat_te

# create the hte object using Random Forests (RF)
xl_rf <- X_RF(feat = feat, tr = tr, yobs = yobs)
tl_rf <- T_RF(feat = feat, tr = tr, yobs = yobs)
sl_rf <- S_RF(feat = feat, tr = tr, yobs = yobs)
ml_rf <- M_RF(feat = feat, tr = tr, yobs = yobs)
xl_bt <- X_BART(feat = feat, tr = tr, yobs = yobs)
tl_bt <- T_BART(feat = feat, tr = tr, yobs = yobs)
sl_bt <- S_BART(feat = feat, tr = tr, yobs = yobs)
ml_bt <- M_BART(feat = feat, tr = tr, yobs = yobs)
  
cate_esti_xrf <- EstimateCate(xl_rf, feature_test)

# evaluate the performance
cate_true <- simulated_experiment$tau_te
mean((cate_esti_xrf - cate_true) ^ 2)
\dontrun{
# create confidence intervals via bootstrapping. 
xl_ci_rf <- CateCI(xl_rf, feature_test, B = 500)
}
}
\references{
\itemize{
  \item Sören Künzel, Jasjeet Sekhon, Peter Bickel, and Bin Yu (2017). 
    MetaLearners for estimating heterogeneous treatment effects using
    machine learning. 
    \url{https://www.pnas.org/content/116/10/4156}
  \item 
    Sören Künzel, Simon Walter, and Jasjeet Sekhon (2018).
    Causaltoolbox---Estimator Stability for Heterogeneous Treatment Effects.
    \url{https://arxiv.org/pdf/1811.02833.pdf}
  \item Daniel Rubin and Mark J van der Laan (2007). A doubly robust
  censoring unbiased transformation.
    \url{https://www.ncbi.nlm.nih.gov/pubmed/22550646}
  }
}
\seealso{
Other metalearners: \code{\link{M_BART}},
  \code{\link{S_BART}}, \code{\link{S_RF}},
  \code{\link{T_BART}}, \code{\link{T_RF}},
  \code{\link{X_BART}}, \code{\link{X_RF}}
}
\concept{metalearners}
